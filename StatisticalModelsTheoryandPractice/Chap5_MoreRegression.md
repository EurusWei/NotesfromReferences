# Highlights
- Make a less restrictive set of assumptions for regression model: $E(\epsilon|X)=0_{n\times 1}, cov(\epsilon|X)=\sigma^2I_{n\times n}$. The OLS estimator is Best Linear Unbiased Estimator (BLUE). (The weaker assumptions do not requie erros to have the same distribution, i.e., a normal distribution and a unifrom distribution can have same mean and variance.)
- Genearlized least squares: let $E(\epsilon|X)=0_{n\times 1}, cov(\epsilon|X)=G$, then $cov(\hat{\beta}_{OLS}|X)=(X^TX)^{-1}X^TGX(X^TX)^{-1}$, and $\hat{\beta}_{OLS}$ is no longer BLUE. To fix this, if $G$ is known, multiply both sides of the model by $G^{-1/2}$, and obtain $\hat{\beta}_{GLS}=(X^TG^{-1}X)^{-1}X^TG^{-1}Y$. $\hat{\beta}_{GLS}$ is conditionally unbiased and BLUE. When $G$ is not known, impose constraints on $G$ to obtain $\hat{\beta}_{FGLS}$, while is usally biased.
- With independent $N(0, \sigma^2)$ errors, the OLS estimator $\hat{\beta}$ has a normal distribution with mean $\beta$ and covariance matrix $\sigma^2(X^TX)^{-1}$. Moreover, $e\perp \hat{\beta}$ and $||e||^2 \sim \sigma^2\chi_d^2$ with $d=n-p$, (the last point is proved by showing that $e$ is equivalent to $\epsilon$ multiplying an orthgonal matrix.)
- F test to test if $p_0$ number of coefficients shall be $0$: $$F = \frac{(||X\hat{\beta}||^2 - ||X\hat{\beta}^{(s)}||^2)/ p_0}{||e||^2 / (n - p)}.$$ Related theorem: with independent $N(0, \sigma^2)$ errors, under the null hypothesis, $$||X\hat{\beta}||^2 - ||X\hat{\beta}^{(s)}||^2\sim U, ||e||^2 \sim V, F\sim\frac{U/p_0}{V/(n-p)},$$ where $U\perp V$, $U\sim\sigma^2\chi^2_{p_0}$, and $V\sim\sigma^2\chi^2_{n-p}$. Intuitively, if the null hypothesis is right, numerator and denominator are both estimating $\sigma^2$, so $F$ should be around $1$. If $p_0$ and $p$ are fixed while $n$ gets large, and the design matrix behaves itself, the normality assumption is not too important. If $p_0, p, n-p$ are similar in size, normality may be an issue.
- Significance only means: if model is right and the coefficients are $0$, it was very unlikely to get such a big $F$-statistic.
- The null hypothesis must involve a statement about a model. Commonly, the null restricts a parameter in the model, instead of relative frequencies in teh data. The data are used to test the null, not to formulate the null.